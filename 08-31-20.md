# August 31st, 2020

- Today, I had 2 goals:
  1. Fix my dataGenerator.js file so that it generated documents with unique ideas.
  2. Optimize my seedDatabase.js file so that it could seed 1 Million of each type of document

Data Generator
- I realized that I could pass in the fileName and then add ids to that so that they would pick up where the last file left off
- To make this work, it was easiest to rename the files to start at 0.

Seed Database
- I really struggled to scale this to be able to handle 1 Million documents.  I was using a library called Mongo Seeding, but it would try to load all the files at once into a variable called collections and overflow the Javascript heap.
- I tried just one file of 100,000 at a time with Mongo Seeding to see if it could handle that, and it could
- I realized that the very specific file structure Mongo Seeding is expecting would make it difficult to feed it one file at a time.  It wants each collection stored in a file with the name of the collection and then those collection folders stored in data.  You pass just the data path to it, so everything in it will be loaded at once.  If I tried to put each file in it's own file to be loaded separately, I would need to have duplicates of each collection and duplicates of the data file.  So I decided to look for something that would be a better fit for my needs
- Next, I tried to see if there was a Mongoose method that could accomplish the goal of loading one large file at a time.  I tried Mongoose's insertMany method, but it couldn't handle a file that large.  I discovered that Mongoose does not have a function that can handle large files like that.
- Since Mongoose couldn't help me out, I next turned to using Mongo methods directly.  You can access a similar Mongo method, insertMany, by doing Products.collection.insertMany.  This worked for my file of 100,000!
- Now I had to figure out how to actually load 10 separate files.  I tried looping to load each document, but that did not work with the async functions, even with await.
- I then tried Promise.all, which worked for 10 product files.  However, it overflowed the heap when I tried to do it for the styles files, which are larger.  It only worked for 4.  I am guessing that it definitely would not work for 100 files, which I will need when I try to load 10M documents.

Tomorrow, I am going to do some more research on async await and see if there is a way I could use that inside of a for loop.  I am going to also think of whether there is a way I can chain 10 promises without hardcoding it. I may also try the loading 1 large file of 1 Million documents into the database instead, maybe through a read stream.